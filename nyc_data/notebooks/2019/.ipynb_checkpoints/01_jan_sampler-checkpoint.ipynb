{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07eac4c-51f8-4d72-9340-5f618243b376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/10 10:03:16 WARN Utils: Your hostname, LAPTOP-VD4O2HIL resolves to a loopback address: 127.0.1.1; using 172.17.55.88 instead (on interface eth0)\n",
      "21/11/10 10:03:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/11/10 10:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# import pyspark to process large files and create a new spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, asc, desc, to_timestamp\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .appName('process_tripdata')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b766dc4-06c3-4a8a-baab-95f66f10afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 2019-01 data\n",
    "jan_data = spark\\\n",
    "    .read\\\n",
    "    .csv('/home/felipe/repos/tcc/nyc_data/csv/2019/fhv_tripdata_2019-01.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5da7df-00c5-4b98-94ba-bd280a647196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter off null PU and DO location IDs\n",
    "jan_data.createOrReplaceTempView(\"JAN_DATA_TEMP_VIEW\")\n",
    "jan_data = spark.sql(\"SELECT * FROM JAN_DATA_TEMP_VIEW WHERE PULocationID IS NOT NULL AND DOLocationID IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0819443-fe79-4047-bdc8-3f7806889232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PU and DO location ID cols to int\n",
    "jan_data = jan_data.withColumn(\"PULocationID\", jan_data.PULocationID.cast('int'))\n",
    "jan_data = jan_data.withColumn(\"DOLocationID\", jan_data.DOLocationID.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478a6eca-2d68-440d-9162-ccf6870716f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trips that did not depart from or arrive at unknown zones\n",
    "jan_data = jan_data.where(\"PULocationID < 264 AND DOLocationID < 264\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c584c8-a78d-4cc6-8394-3d52f4660852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep rows where PU and DO cols are not 0\n",
    "jan_data.createOrReplaceTempView(\"JAN_DATA_TEMP_VIEW\")\n",
    "jan_data = spark.sql(\"SELECT * FROM JAN_DATA_TEMP_VIEW WHERE PULocationID <> 0 AND DOLocationID <> 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10033a4-efba-4d93-8436-5dd7e2476e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:================================>                         (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan data count 19847263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"jan data count\", jan_data.count()) # 19847263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d99af3ca-cfa4-43db-b879-45fea38225c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1% of the set\n",
    "jan_data_sample = jan_data.sample(fraction=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6bd2fe-ea1a-45bf-bcf3-2eb841112e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan sample count 198615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"jan sample count\", jan_data_sample.count()) # 198615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f102bb55-c521-47c5-9d84-eca0391f6552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jan_data_sample.repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/home/felipe/repos/tcc/nyc_data/csv/201901_sample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
