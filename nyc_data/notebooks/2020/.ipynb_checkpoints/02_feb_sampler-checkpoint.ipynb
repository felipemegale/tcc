{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40408285-0df2-433c-b63c-7b1101438f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/10 10:36:31 WARN Utils: Your hostname, LAPTOP-VD4O2HIL resolves to a loopback address: 127.0.1.1; using 172.17.55.88 instead (on interface eth0)\n",
      "21/11/10 10:36:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/11/10 10:36:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/10 10:36:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/11/10 10:36:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/11/10 10:36:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "21/11/10 10:36:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "# import pyspark to process large files and create a new spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, asc, desc, to_timestamp\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .appName('process_tripdata')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111a38fc-0e42-4c79-8d4f-88c2265b80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 2020-02 data\n",
    "fhv_data = spark\\\n",
    "    .read\\\n",
    "    .csv('/home/felipe/repos/tcc/nyc_data/csv/2020/fhv_tripdata_2020-02.csv', header=True)\n",
    "fhvhv_data = spark\\\n",
    "    .read\\\n",
    "    .csv('/home/felipe/repos/tcc/nyc_data/csv/2020/fhvhv_tripdata_2020-02.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e64e83-216e-4020-9053-ee20f0dc7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Affiliated_base_number column from fhv_data\n",
    "fhv_data = fhv_data.drop(\"Affiliated_base_number\")\n",
    "\n",
    "# drop hvfhs_license_num column from fhvhv_data\n",
    "fhvhv_data = fhvhv_data.drop(\"hvfhs_license_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3044e08e-efeb-4e1a-a8fc-a17a9144c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter off null PU and DO location IDs\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_data = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE PULocationID IS NOT NULL AND DOLocationID IS NOT NULL\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_data = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE PULocationID IS NOT NULL AND DOLocationID IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d779822-d074-4ec0-9b39-ecf311c8f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PU and DO location ID cols to int\n",
    "fhv_data = fhv_data.withColumn(\"PULocationID\", fhv_data.PULocationID.cast('int'))\n",
    "fhv_data = fhv_data.withColumn(\"DOLocationID\", fhv_data.DOLocationID.cast('int'))\n",
    "\n",
    "# convert PU and DO location ID cols to int\n",
    "fhvhv_data = fhvhv_data.withColumn(\"PULocationID\", fhvhv_data.PULocationID.cast('int'))\n",
    "fhvhv_data = fhvhv_data.withColumn(\"DOLocationID\", fhvhv_data.DOLocationID.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd3c12c-c5c2-42f3-842e-da7e1f6d95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trips that did not depart from or arrive at unknown zones\n",
    "fhv_data = fhv_data.where(\"PULocationID < 264 AND DOLocationID < 264\")\n",
    "fhvhv_data = fhvhv_data.where(\"PULocationID < 264 AND DOLocationID < 264\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41e67fb-d2f6-4e90-9dc2-c7104a05e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep rows where PU and DO cols are not 0\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_data = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE PULocationID <> 0 AND DOLocationID <> 0\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_data = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE PULocationID <> 0 AND DOLocationID <> 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be22e2df-2493-4bee-9a9f-7e4bf11ee6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhv data count 363166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhvhv data count 21057911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"fhv data count\", fhv_data.count()) # 363166\n",
    "print(\"fhvhv data count\", fhvhv_data.count()) # 21057911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a69f391-3853-4dc0-9187-732aba68a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1% of the set\n",
    "fhv_sample = fhv_data.sample(fraction=0.01)\n",
    "fhvhv_sample = fhvhv_data.sample(fraction=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041bf4fb-b312-473a-bff3-29daa9a25c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================>                   (21 + 11) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feb sample count 214980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feb_sample = fhv_sample.union(fhvhv_sample)\n",
    "\n",
    "print(\"feb sample count\", feb_sample.count()) # 214980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a63cf0-2c51-4335-aaf5-e01094211bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# append new data to sampled set\n",
    "feb_sample.repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/home/felipe/repos/tcc/nyc_data/csv/202002_sample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
