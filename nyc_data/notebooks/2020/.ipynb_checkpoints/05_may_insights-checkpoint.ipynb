{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95c251-4b9f-45fe-9395-4594373fd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark to process large files and create a new spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, asc, desc, to_timestamp, when\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .appName('process_tripdata')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a4d62-4cb9-4d76-9ea6-59b720f8b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 2019-05 data\n",
    "fhv_data = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/2020/05/fhv_tripdata_2020-05.csv', header=True)\n",
    "fhvhv_data = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/2020/05/fhvhv_tripdata_2020-05.csv', header=True)\n",
    "\n",
    "# load taxi zones\n",
    "taxi_zones = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/taxi_zone_lookup.csv', header=True)\n",
    "\n",
    "# load app lookup\n",
    "apps_lookup = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/hvfhs_licenses.csv', header=True)\n",
    "\n",
    "# load NYC shape file to geopandas\n",
    "gdf = geopandas.read_file('/media/felipe/Files/repos/tcc/nyc_data/shapes/taxi_zones/taxi_zones.dbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71a2d8-f807-41d9-a724-470ecb401f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mar data count -> 1,908,099 + 21,734,767\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab1c5e-679f-47f2-a142-9c05920317ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter off null PU and DO location IDs\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_data = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE PULocationID IS NOT NULL AND DOLocationID IS NOT NULL\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_data = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE PULocationID IS NOT NULL AND DOLocationID IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088a2c4-d391-48ea-a3a1-9f99ab707071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get apr data count after filtering -> 1,908,099 + 21,734,767\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4649a-20b9-4b8f-98f5-0f8d45926481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PU and DO location ID cols to int and filter off unknown locations\n",
    "fhv_data = fhv_data\\\n",
    "    .withColumn(\"PULocationID\", fhv_data.PULocationID.cast('int'))\\\n",
    "    .withColumn(\"DOLocationID\", fhv_data.DOLocationID.cast('int'))\n",
    "fhv_data = fhv_data.where(\"PULocationID < 264 AND DOLocationID < 264\")\n",
    "\n",
    "fhvhv_data = fhvhv_data\\\n",
    "    .withColumn(\"PULocationID\", fhvhv_data.PULocationID.cast('int'))\\\n",
    "    .withColumn(\"DOLocationID\", fhvhv_data.DOLocationID.cast('int'))\n",
    "fhvhv_data = fhvhv_data.where(\"PULocationID < 264 AND DOLocationID < 264\")\n",
    "\n",
    "# count how many trips did not depart from or arrive to unknown zones -> 237,815 + 21,059,243\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61157073-9562-4b26-b615-922cb5f37aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where PU and DO cols are 0\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_data = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE PULocationID <> 0 AND DOLocationID <> 0\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_data = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE PULocationID <> 0 AND DOLocationID <> 0\")\n",
    "\n",
    "# count how many trips PU and DO locations are 0 -> 237,815 + 21,059,243\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97b125-3a76-47e6-8c7c-2469b3805692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OD matrix\n",
    "od_size = 264 # 263 + 1 to account for 0-index in list\n",
    "od_data = [[0 for x in range(od_size)] for y in range(od_size)]\n",
    "od_cols = [str(x) for x in range(od_size)]\n",
    "\n",
    "# fill feb OD matrix dataframe\n",
    "fhv_data_collect = fhv_data.toLocalIterator()\n",
    "for fhv_row in fhv_data_collect:\n",
    "    origin = fhv_row.PULocationID\n",
    "    destination = fhv_row.DOLocationID\n",
    "    od_data[origin][destination] += 1\n",
    "    \n",
    "fhvhv_data_collect = fhvhv_data.toLocalIterator()\n",
    "for fhvhv_row in fhvhv_data_collect:\n",
    "    origin = fhvhv_row.PULocationID\n",
    "    destination = fhvhv_row.DOLocationID\n",
    "    od_data[origin][destination] += 1\n",
    "    \n",
    "# create OD dataframe from OD matrix\n",
    "od_dataframe = spark.createDataFrame(data=od_data,schema=od_cols)\n",
    "\n",
    "# create and populate list of max trips from to location\n",
    "od_dataframe_collect = od_dataframe.toLocalIterator()\n",
    "origin = 0\n",
    "od_greatest_values = []\n",
    "\n",
    "for row in od_dataframe_collect:\n",
    "    max_val = max(list(row))\n",
    "    destination = row.index(max_val)\n",
    "    od_greatest_values.append((origin, destination, max_val))\n",
    "    origin += 1\n",
    "\n",
    "# create dataframe from od_greatest_values list\n",
    "od_gr_df_cols = [\"PULocationID\", \"DOLocationID\", \"TripQty\"]\n",
    "od_gr_df = spark.createDataFrame(data=od_greatest_values, schema=od_gr_df_cols)\n",
    "\n",
    "# order by TripQty to find find the top destinations\n",
    "od_gr_df = od_gr_df.orderBy(col(\"TripQty\").desc())\n",
    "od_gr_df.show(10)\n",
    "\n",
    "# extract may top OD data\n",
    "top_ods = od_gr_df\\\n",
    "    .join(taxi_zones,od_gr_df.PULocationID == taxi_zones.LocationID, \"inner\")\\\n",
    "    .drop(\"Borough\", \"service_zone\")\n",
    "\n",
    "# save may top ODs to file\n",
    "try:\n",
    "    top_ods\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/05/top_ods.csv\")\n",
    "except:\n",
    "    print(\"Could not write top_ods to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017cddc-fe82-4e11-8a42-2751798c110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up helper lists for trip division by time of day\n",
    "fhv_pickup_times = fhv_data.select(\"pickup_datetime\")\n",
    "fhv_pickup_times = fhv_pickup_times.withColumn(\"pickup_datetime\",to_timestamp(\"pickup_datetime\"))\n",
    "\n",
    "# print(\"!!!!! FHV !!!!!\")\n",
    "# fhv_pickup_times.printSchema()\n",
    "# fhv_pickup_times.show(5)\n",
    "\n",
    "fhvhv_pickup_times = fhvhv_data.select(\"pickup_datetime\")\n",
    "fhvhv_pickup_times = fhvhv_pickup_times.withColumn(\"pickup_datetime\",to_timestamp(\"pickup_datetime\"))\n",
    "\n",
    "# print(\"!!!!! FHVHV !!!!!\")\n",
    "# fhvhv_pickup_times.printSchema()\n",
    "# fhvhv_pickup_times.show(5)\n",
    "\n",
    "# count how many trips per day period\n",
    "# morning: 0:00 to 5:59\n",
    "# day: 6:00 to 11:59\n",
    "# noon: 12:00 to 17:59\n",
    "# night: 18:00 to 23:59\n",
    "# weekday is 0-index starting monday (0), then tuesday (1), etc.\n",
    "\n",
    "MORNING=1\n",
    "DAY=2\n",
    "NOON=3\n",
    "NIGHT=4\n",
    "trip_groups_schema = ['day of week', 'morning', 'day', 'noon', 'night']\n",
    "trip_groups = [['mon', 0, 0, 0, 0],\\\n",
    "               ['tue', 0, 0, 0, 0],\\\n",
    "               ['wed', 0, 0, 0, 0],\\\n",
    "               ['thu', 0, 0, 0, 0],\\\n",
    "               ['fri', 0, 0, 0, 0],\\\n",
    "               ['sat', 0, 0, 0, 0],\\\n",
    "               ['sun', 0, 0, 0, 0]]\n",
    "\n",
    "fhv_pickup_times_collect = fhv_pickup_times.toLocalIterator()\n",
    "\n",
    "for row in fhv_pickup_times_collect:\n",
    "    trip_weekday = row.pickup_datetime.weekday()\n",
    "    trip_hour = row.pickup_datetime.hour\n",
    "    if 0 <= trip_hour <= 5:\n",
    "        trip_groups[trip_weekday][MORNING] += 1\n",
    "    elif 6 <= trip_hour <= 11:\n",
    "        trip_groups[trip_weekday][DAY] += 1\n",
    "    elif 12 <= trip_hour <= 17:\n",
    "        trip_groups[trip_weekday][NOON] += 1\n",
    "    elif 18 <= trip_hour <= 23:\n",
    "        trip_groups[trip_weekday][NIGHT] += 1\n",
    "\n",
    "fhvhv_pickup_times_collect = fhvhv_pickup_times.toLocalIterator()\n",
    "\n",
    "for row in fhvhv_pickup_times_collect:\n",
    "    trip_weekday = row.pickup_datetime.weekday()\n",
    "    trip_hour = row.pickup_datetime.hour\n",
    "    if 0 <= trip_hour <= 5:\n",
    "        trip_groups[trip_weekday][MORNING] += 1\n",
    "    elif 6 <= trip_hour <= 11:\n",
    "        trip_groups[trip_weekday][DAY] += 1\n",
    "    elif 12 <= trip_hour <= 17:\n",
    "        trip_groups[trip_weekday][NOON] += 1\n",
    "    elif 18 <= trip_hour <= 23:\n",
    "        trip_groups[trip_weekday][NIGHT] += 1\n",
    "        \n",
    "# create DF from trip dow group matrix\n",
    "trip_dow_df = spark.createDataFrame(data=trip_groups, schema=trip_groups_schema)\n",
    "\n",
    "# show trip_dow_df\n",
    "trip_dow_df.show()\n",
    "\n",
    "# write trip_dow_df to file\n",
    "try:\n",
    "    trip_dow_df\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/05/trip_dow_time.csv\")\n",
    "except:\n",
    "    print(\"Could not write trip_dow_time to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cc9df-0a98-48a2-8879-d5980d7c6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trips that were shared\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_shared_trips = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE SR_Flag IS NOT NULL\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_shared_trips = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE SR_Flag IS NOT NULL\")\n",
    "\n",
    "# count how many trips were shared -> 0 + 3,826,284\n",
    "\n",
    "# As per the documentation, there are trips that were flagged as shareable.\n",
    "# however, this does not mean that it was, since Lyft flags as shared even though\n",
    "# the original rider wasnt matched with someone else.\n",
    "# For the purposes of this study, we will analyze the users intent to share.\n",
    "\n",
    "print('shared fhv trips', fhv_shared_trips.count())\n",
    "print('shared fhvhv trips', fhvhv_shared_trips.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77566cd-f3e1-467e-a4ed-8a27b3afac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count top departure, arrival zones and write to file\n",
    "pu_data = fhv_data\\\n",
    "    .select(\"PULocationID\")\\\n",
    "    .union(fhvhv_data\\\n",
    "           .select(\"PULocationID\"))\n",
    "\n",
    "trips_by_pu = pu_data\\\n",
    "    .groupBy(\"PULocationID\")\\\n",
    "    .count()\\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "try:\n",
    "    trips_by_pu\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/05/trips_by_pu.csv\")\n",
    "except:\n",
    "    print(\"Could not write trips_by_pu to file or file already exists\")\n",
    "\n",
    "do_data = fhv_data\\\n",
    "    .select(\"DOLocationID\")\\\n",
    "    .union(fhvhv_data\\\n",
    "           .select(\"DOLocationID\"))\n",
    "\n",
    "trips_by_do = do_data\\\n",
    "    .groupBy(\"DOLocationID\")\\\n",
    "    .count()\\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "try:\n",
    "    trips_by_do\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/05/trips_by_do.csv\")\n",
    "except:\n",
    "    print(\"Could not write trips_by_do to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21104e7-c319-4fca-8ec1-c31e0daca795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get what app is most used to share rides\n",
    "shared_trips = fhvhv_data.where(\"SR_Flag IS NOT NULL\")\n",
    "shared_trips = shared_trips.drop(\"dispatching_base_num\")\n",
    "shared_trips = shared_trips\\\n",
    "    .join(apps_lookup,\n",
    "          shared_trips.hvfhs_license_num == apps_lookup.hvfhs_license_num)\\\n",
    "    .drop(\"hvfhs_license_num\")\n",
    "\n",
    "shared_trips = shared_trips\\\n",
    "    .groupBy(\"company_name\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"count\")\n",
    "\n",
    "shared_trips.show()\n",
    "\n",
    "# write shared_trips to file\n",
    "try:\n",
    "    shared_trips\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/05/trips_shared_by_app.csv\")\n",
    "except:\n",
    "    print(\"Could not write trips_shared_by_app to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767c289-4ee1-4b61-80f3-e7e2241f069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find missing lines from pu and do lists\n",
    "all_location_ids = taxi_zones\\\n",
    "    .select(\"LocationID\")\\\n",
    "    .where(\"LocationID < 264\")\\\n",
    "    .withColumn(\"LocationID\", taxi_zones.LocationID.cast(\"int\"))\n",
    "all_location_ids = np.array(all_location_ids.collect()).reshape(-1)\n",
    "\n",
    "location_ids_pu = np.array(trips_by_pu.select(\"PULocationID\").collect()).reshape(-1)\n",
    "location_ids_do = np.array(trips_by_do.select(\"DOLocationID\").collect()).reshape(-1)\n",
    "\n",
    "pu_schema = [\"PULocationID\", \"count\"]\n",
    "do_schema = [\"DOLocationID\", \"count\"]\n",
    "missing_locations_pu = []\n",
    "missing_locations_do = []\n",
    "\n",
    "for location_id in np.nditer(all_location_ids):\n",
    "    if location_id not in location_ids_pu:\n",
    "        missing_locations_pu.append((location_id.flatten().item(0),0))\n",
    "    if location_id not in location_ids_do:\n",
    "        missing_locations_do.append((location_id.flatten().item(0),0))\n",
    "\n",
    "missing_pu_data = spark.createDataFrame(data=missing_locations_pu,schema=pu_schema)\n",
    "trips_by_pu = trips_by_pu\\\n",
    "    .union(missing_pu_data)\\\n",
    "    .orderBy(\"PULocationID\")\n",
    "trips_by_pu_count = np.array(trips_by_pu.select(\"count\").collect()).reshape(-1)\n",
    "\n",
    "missing_do_data = spark.createDataFrame(data=missing_locations_do,schema=do_schema)\n",
    "trips_by_do = trips_by_do\\\n",
    "    .union(missing_do_data)\\\n",
    "    .orderBy(\"DOLocationID\")\n",
    "trips_by_do_count = np.array(trips_by_do.select(\"count\").collect()).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3fd9b-8e99-4867-b050-141329bb2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df with company name instead of license number\n",
    "fhvhv_data_cmp_name = fhvhv_data\\\n",
    "    .join(apps_lookup,\n",
    "          fhvhv_data.hvfhs_license_num == apps_lookup.hvfhs_license_num)\\\n",
    "    .withColumn(\"PULocationID\", fhvhv_data.PULocationID.cast(\"int\"))\\\n",
    "    .withColumn(\"DOLocationID\", fhvhv_data.DOLocationID.cast(\"int\"))\\\n",
    "    .drop(\"hvfhs_license_num\", \"dispatching_base_num\")\n",
    "\n",
    "fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "    .groupBy(\"PULocationID\", \"company_name\", \"company_color\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"PULocationID\")\n",
    "\n",
    "fhvhv_data_cmp_name.createOrReplaceTempView(\"TMP_VIEW\")\n",
    "\n",
    "query = \"SELECT PULocationID, company_name, company_color, count FROM (SELECT *, MAX(count) OVER (PARTITION BY PULocationID) AS maxCount FROM TMP_VIEW) M WHERE count = maxCount\"\n",
    "\n",
    "fhvhv_data_cmp_name = spark.sql(query)\n",
    "\n",
    "# fhvhv_data_cmp_name.show(110)\n",
    "\n",
    "# fixture specific for may\n",
    "# fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "#     .withColumn(\"PULocationID\", when((col(\"PULocationID\") == 105) & (col(\"company_name\") == 'Lyft'), 104)\\\n",
    "#                .otherwise(col(\"PULocationID\")))\n",
    "# fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "#     .withColumn(\"PULocationID\", when((col(\"PULocationID\") == 105) & (col(\"company_name\") == 'Juno'), 103)\\\n",
    "#                .otherwise(col(\"PULocationID\")))\n",
    "# fixture specific for may\n",
    "\n",
    "# find missing PULocationID in fhvhv_data_cmp_name\n",
    "pu_schema = [\"PULocationID\", \"company_name\", \"company_color\", \"count\"]\n",
    "missing_locations_pu = []\n",
    "\n",
    "location_ids_pu = np.array(fhvhv_data_cmp_name.select(\"PULocationID\").collect()).reshape(-1)\n",
    "\n",
    "for location_id in np.nditer(all_location_ids):\n",
    "    if location_id not in location_ids_pu:\n",
    "        missing_locations_pu.append((location_id.flatten().item(0),'No data','#808080',np.nan))\n",
    "\n",
    "missing_pu_data = spark.createDataFrame(data=missing_locations_pu,schema=pu_schema)\n",
    "fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "    .union(missing_pu_data)\\\n",
    "    .orderBy(\"PULocationID\")\n",
    "\n",
    "fhvhv_data_cmp_color = np.array(fhvhv_data_cmp_name.select('company_color').collect()).reshape(-1)\n",
    "fhvhv_data_cmp_name_name = np.array(fhvhv_data_cmp_name.select('company_name').collect()).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c978f81-cd35-4efa-80c2-3814a02b52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cols to gdf\n",
    "gdf['Trips_by_PULocationID'] = trips_by_pu_count\n",
    "gdf['Trips_by_DOLocationID'] = trips_by_do_count\n",
    "gdf['Top_Company_Color'] = fhvhv_data_cmp_color\n",
    "gdf['Top_Company_Name'] = fhvhv_data_cmp_name_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08a829-c5bf-4722-b379-a1b0d376d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PU trip heatmap\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.title('NYC, May 2020')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gdf.plot(cmap='coolwarm',\n",
    "         column='Trips_by_PULocationID',\n",
    "         ax=ax,\n",
    "         legend=True,\n",
    "         legend_kwds={'label': \"Pickups by Zone\", 'orientation': \"vertical\"})\n",
    "plt.savefig('/media/felipe/Files/repos/tcc/nyc_data/imgs/2020-05_PU.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42c563-ab44-4ca2-8fd7-cb938bac3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot DO trip heatmap\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.title('NYC, May 2020')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gdf.plot(cmap='coolwarm',\n",
    "         column='Trips_by_DOLocationID',\n",
    "         ax=ax,\n",
    "         legend=True,\n",
    "         legend_kwds={'label': \"Dropoffs by Zone\", 'orientation': \"vertical\"})\n",
    "plt.savefig('/media/felipe/Files/repos/tcc/nyc_data/imgs/2020-05_DO.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbba7b-9bfe-413b-aeda-ec9a18507a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top companies\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.title('Most used app by NYC zone, May 2020')\n",
    "uber_patch = mpatches.Patch(color='#000000', label='Uber')\n",
    "lyft_patch = mpatches.Patch(color='#E867CB', label='Lyft')\n",
    "via_patch = mpatches.Patch(color='#2AB6E6', label='Via')\n",
    "juno_patch = mpatches.Patch(color='#29509F', label='Juno')\n",
    "no_data_patch = mpatches.Patch(color='#808080', label='No data')\n",
    "plt.legend(handles=[uber_patch,lyft_patch,via_patch,juno_patch], loc='upper left')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gdf.plot(color=gdf['Top_Company_Color'],\n",
    "         ax=ax,\n",
    "         legend=True)\n",
    "plt.savefig('/media/felipe/Files/repos/tcc/nyc_data/imgs/2020-05_App.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbcc06-0466-42d0-88f5-b98200a927dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
