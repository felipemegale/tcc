{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f95c251-4b9f-45fe-9395-4594373fd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark to process large files and create a new spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, asc, desc, to_timestamp, when\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .appName('process_tripdata')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768a4d62-4cb9-4d76-9ea6-59b720f8b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 2019-06 data\n",
    "fhv_data = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/2020/11/fhv_tripdata_2020-11.csv', header=True)\n",
    "fhvhv_data = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/2020/11/fhvhv_tripdata_2020-11.csv', header=True)\n",
    "\n",
    "# load taxi zones\n",
    "taxi_zones = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/taxi_zone_lookup.csv', header=True)\n",
    "\n",
    "# load app lookup\n",
    "apps_lookup = spark\\\n",
    "    .read\\\n",
    "    .csv('/media/felipe/Files/repos/tcc/nyc_data/csv/hvfhs_licenses.csv', header=True)\n",
    "\n",
    "# load NYC shape file to geopandas\n",
    "gdf = geopandas.read_file('/media/felipe/Files/repos/tcc/nyc_data/shapes/taxi_zones/taxi_zones.dbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b71a2d8-f807-41d9-a724-470ecb401f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mar data count -> 1,908,099 + 21,734,767\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fab1c5e-679f-47f2-a142-9c05920317ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter off null PU and DO location IDs\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_data = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE PULocationID IS NOT NULL AND DOLocationID IS NOT NULL\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_data = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE PULocationID IS NOT NULL AND DOLocationID IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8088a2c4-d391-48ea-a3a1-9f99ab707071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get apr data count after filtering -> 1,908,099 + 21,734,767\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f4649a-20b9-4b8f-98f5-0f8d45926481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PU and DO location ID cols to int and filter off unknown locations\n",
    "fhv_data = fhv_data\\\n",
    "    .withColumn(\"PULocationID\", fhv_data.PULocationID.cast('int'))\\\n",
    "    .withColumn(\"DOLocationID\", fhv_data.DOLocationID.cast('int'))\n",
    "fhv_data = fhv_data.where(\"PULocationID < 264 AND DOLocationID < 264\")\n",
    "\n",
    "fhvhv_data = fhvhv_data\\\n",
    "    .withColumn(\"PULocationID\", fhvhv_data.PULocationID.cast('int'))\\\n",
    "    .withColumn(\"DOLocationID\", fhvhv_data.DOLocationID.cast('int'))\n",
    "fhvhv_data = fhvhv_data.where(\"PULocationID < 264 AND DOLocationID < 264\")\n",
    "\n",
    "# count how many trips did not depart from or arrive to unknown zones -> 237,815 + 21,059,243\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61157073-9562-4b26-b615-922cb5f37aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where PU and DO cols are 0\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_data = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE PULocationID <> 0 AND DOLocationID <> 0\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_data = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE PULocationID <> 0 AND DOLocationID <> 0\")\n",
    "\n",
    "# count how many trips PU and DO locations are 0 -> 237,815 + 21,059,243\n",
    "# print('fhv', fhv_data.count())\n",
    "# print('fhvhv', fhvhv_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d97b125-3a76-47e6-8c7c-2469b3805692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------+\n",
      "|PULocationID|DOLocationID|TripQty|\n",
      "+------------+------------+-------+\n",
      "|          76|          76|  43609|\n",
      "|          26|          26|  26555|\n",
      "|          39|          39|  26080|\n",
      "|          61|          61|  25387|\n",
      "|          14|          14|  15874|\n",
      "|          42|          42|  15033|\n",
      "|         129|         129|  13943|\n",
      "|          37|          37|  13900|\n",
      "|           7|           7|  13845|\n",
      "|          89|          89|  13162|\n",
      "+------------+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Could not write top_ods to file or file already exists\n"
     ]
    }
   ],
   "source": [
    "# OD matrix\n",
    "od_size = 264 # 263 + 1 to account for 0-index in list\n",
    "od_data = [[0 for x in range(od_size)] for y in range(od_size)]\n",
    "od_cols = [str(x) for x in range(od_size)]\n",
    "\n",
    "# fill feb OD matrix dataframe\n",
    "fhv_data_collect = fhv_data.toLocalIterator()\n",
    "for fhv_row in fhv_data_collect:\n",
    "    origin = fhv_row.PULocationID\n",
    "    destination = fhv_row.DOLocationID\n",
    "    od_data[origin][destination] += 1\n",
    "    \n",
    "fhvhv_data_collect = fhvhv_data.toLocalIterator()\n",
    "for fhvhv_row in fhvhv_data_collect:\n",
    "    origin = fhvhv_row.PULocationID\n",
    "    destination = fhvhv_row.DOLocationID\n",
    "    od_data[origin][destination] += 1\n",
    "    \n",
    "# create OD dataframe from OD matrix\n",
    "od_dataframe = spark.createDataFrame(data=od_data,schema=od_cols)\n",
    "\n",
    "# create and populate list of max trips from to location\n",
    "od_dataframe_collect = od_dataframe.toLocalIterator()\n",
    "origin = 0\n",
    "od_greatest_values = []\n",
    "\n",
    "for row in od_dataframe_collect:\n",
    "    max_val = max(list(row))\n",
    "    destination = row.index(max_val)\n",
    "    od_greatest_values.append((origin, destination, max_val))\n",
    "    origin += 1\n",
    "\n",
    "# create dataframe from od_greatest_values list\n",
    "od_gr_df_cols = [\"PULocationID\", \"DOLocationID\", \"TripQty\"]\n",
    "od_gr_df = spark.createDataFrame(data=od_greatest_values, schema=od_gr_df_cols)\n",
    "\n",
    "# order by TripQty to find find the top destinations\n",
    "od_gr_df = od_gr_df.orderBy(col(\"TripQty\").desc())\n",
    "od_gr_df.show(10)\n",
    "\n",
    "# extract may top OD data\n",
    "top_ods = od_gr_df\\\n",
    "    .join(taxi_zones,od_gr_df.PULocationID == taxi_zones.LocationID, \"inner\")\\\n",
    "    .drop(\"Borough\", \"service_zone\")\n",
    "\n",
    "# save may top ODs to file\n",
    "try:\n",
    "    top_ods\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/11/top_ods.csv\")\n",
    "except:\n",
    "    print(\"Could not write top_ods to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6017cddc-fe82-4e11-8a42-2751798c110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+------+------+\n",
      "|day of week|morning|   day|  noon| night|\n",
      "+-----------+-------+------+------+------+\n",
      "|        mon| 143051|527380|560447|441588|\n",
      "|        tue| 106574|389209|473571|417609|\n",
      "|        wed| 123234|391631|492394|436661|\n",
      "|        thu| 123017|374004|494591|502760|\n",
      "|        fri| 171041|393663|513260|561505|\n",
      "|        sat| 216759|308598|568129|627672|\n",
      "|        sun| 411107|340235|618175|498885|\n",
      "+-----------+-------+------+------+------+\n",
      "\n",
      "Could not write trip_dow_time to file or file already exists\n"
     ]
    }
   ],
   "source": [
    "# set up helper lists for trip division by time of day\n",
    "fhv_pickup_times = fhv_data.select(\"pickup_datetime\")\n",
    "fhv_pickup_times = fhv_pickup_times.withColumn(\"pickup_datetime\",to_timestamp(\"pickup_datetime\"))\n",
    "\n",
    "# print(\"!!!!! FHV !!!!!\")\n",
    "# fhv_pickup_times.printSchema()\n",
    "# fhv_pickup_times.show(5)\n",
    "\n",
    "fhvhv_pickup_times = fhvhv_data.select(\"pickup_datetime\")\n",
    "fhvhv_pickup_times = fhvhv_pickup_times.withColumn(\"pickup_datetime\",to_timestamp(\"pickup_datetime\"))\n",
    "\n",
    "# print(\"!!!!! FHVHV !!!!!\")\n",
    "# fhvhv_pickup_times.printSchema()\n",
    "# fhvhv_pickup_times.show(5)\n",
    "\n",
    "# count how many trips per day period\n",
    "# morning: 0:00 to 5:59\n",
    "# day: 6:00 to 11:59\n",
    "# noon: 12:00 to 17:59\n",
    "# night: 18:00 to 23:59\n",
    "# weekday is 0-index starting monday (0), then tuesday (1), etc.\n",
    "\n",
    "MORNING=1\n",
    "DAY=2\n",
    "NOON=3\n",
    "NIGHT=4\n",
    "trip_groups_schema = ['day of week', 'morning', 'day', 'noon', 'night']\n",
    "trip_groups = [['mon', 0, 0, 0, 0],\\\n",
    "               ['tue', 0, 0, 0, 0],\\\n",
    "               ['wed', 0, 0, 0, 0],\\\n",
    "               ['thu', 0, 0, 0, 0],\\\n",
    "               ['fri', 0, 0, 0, 0],\\\n",
    "               ['sat', 0, 0, 0, 0],\\\n",
    "               ['sun', 0, 0, 0, 0]]\n",
    "\n",
    "fhv_pickup_times_collect = fhv_pickup_times.toLocalIterator()\n",
    "\n",
    "for row in fhv_pickup_times_collect:\n",
    "    trip_weekday = row.pickup_datetime.weekday()\n",
    "    trip_hour = row.pickup_datetime.hour\n",
    "    if 0 <= trip_hour <= 5:\n",
    "        trip_groups[trip_weekday][MORNING] += 1\n",
    "    elif 6 <= trip_hour <= 11:\n",
    "        trip_groups[trip_weekday][DAY] += 1\n",
    "    elif 12 <= trip_hour <= 17:\n",
    "        trip_groups[trip_weekday][NOON] += 1\n",
    "    elif 18 <= trip_hour <= 23:\n",
    "        trip_groups[trip_weekday][NIGHT] += 1\n",
    "\n",
    "fhvhv_pickup_times_collect = fhvhv_pickup_times.toLocalIterator()\n",
    "\n",
    "for row in fhvhv_pickup_times_collect:\n",
    "    trip_weekday = row.pickup_datetime.weekday()\n",
    "    trip_hour = row.pickup_datetime.hour\n",
    "    if 0 <= trip_hour <= 5:\n",
    "        trip_groups[trip_weekday][MORNING] += 1\n",
    "    elif 6 <= trip_hour <= 11:\n",
    "        trip_groups[trip_weekday][DAY] += 1\n",
    "    elif 12 <= trip_hour <= 17:\n",
    "        trip_groups[trip_weekday][NOON] += 1\n",
    "    elif 18 <= trip_hour <= 23:\n",
    "        trip_groups[trip_weekday][NIGHT] += 1\n",
    "        \n",
    "# create DF from trip dow group matrix\n",
    "trip_dow_df = spark.createDataFrame(data=trip_groups, schema=trip_groups_schema)\n",
    "\n",
    "# show trip_dow_df\n",
    "trip_dow_df.show()\n",
    "\n",
    "# write trip_dow_df to file\n",
    "try:\n",
    "    trip_dow_df\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/11/trip_dow_time.csv\")\n",
    "except:\n",
    "    print(\"Could not write trip_dow_time to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0cc9df-0a98-48a2-8879-d5980d7c6c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared fhv trips 0\n",
      "shared fhvhv trips 1020\n"
     ]
    }
   ],
   "source": [
    "# get trips that were shared\n",
    "fhv_data.createOrReplaceTempView(\"FHV_DATA_TEMP_VIEW\")\n",
    "fhv_shared_trips = spark.sql(\"SELECT * FROM FHV_DATA_TEMP_VIEW WHERE SR_Flag IS NOT NULL\")\n",
    "\n",
    "fhvhv_data.createOrReplaceTempView(\"FHVHV_DATA_TEMP_VIEW\")\n",
    "fhvhv_shared_trips = spark.sql(\"SELECT * FROM FHVHV_DATA_TEMP_VIEW WHERE SR_Flag IS NOT NULL\")\n",
    "\n",
    "# count how many trips were shared -> 0 + 3,826,284\n",
    "\n",
    "# As per the documentation, there are trips that were flagged as shareable.\n",
    "# however, this does not mean that it was, since Lyft flags as shared even though\n",
    "# the original rider wasnt matched with someone else.\n",
    "# For the purposes of this study, we will analyze the users intent to share.\n",
    "\n",
    "print('shared fhv trips', fhv_shared_trips.count())\n",
    "print('shared fhvhv trips', fhvhv_shared_trips.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d77566cd-f3e1-467e-a4ed-8a27b3afac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not write trips_by_pu to file or file already exists\n",
      "Could not write trips_by_do to file or file already exists\n"
     ]
    }
   ],
   "source": [
    "# count top departure, arrival zones and write to file\n",
    "pu_data = fhv_data\\\n",
    "    .select(\"PULocationID\")\\\n",
    "    .union(fhvhv_data\\\n",
    "           .select(\"PULocationID\"))\n",
    "\n",
    "trips_by_pu = pu_data\\\n",
    "    .groupBy(\"PULocationID\")\\\n",
    "    .count()\\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "try:\n",
    "    trips_by_pu\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/11/trips_by_pu.csv\")\n",
    "except:\n",
    "    print(\"Could not write trips_by_pu to file or file already exists\")\n",
    "\n",
    "do_data = fhv_data\\\n",
    "    .select(\"DOLocationID\")\\\n",
    "    .union(fhvhv_data\\\n",
    "           .select(\"DOLocationID\"))\n",
    "\n",
    "trips_by_do = do_data\\\n",
    "    .groupBy(\"DOLocationID\")\\\n",
    "    .count()\\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "try:\n",
    "    trips_by_do\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/11/trips_by_do.csv\")\n",
    "except:\n",
    "    print(\"Could not write trips_by_do to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21104e7-c319-4fca-8ec1-c31e0daca795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|company_name|count|\n",
      "+------------+-----+\n",
      "|         Via| 1020|\n",
      "+------------+-----+\n",
      "\n",
      "Could not write trips_shared_by_app to file or file already exists\n"
     ]
    }
   ],
   "source": [
    "# get what app is most used to share rides\n",
    "shared_trips = fhvhv_data.where(\"SR_Flag IS NOT NULL\")\n",
    "shared_trips = shared_trips.drop(\"dispatching_base_num\")\n",
    "shared_trips = shared_trips\\\n",
    "    .join(apps_lookup,\n",
    "          shared_trips.hvfhs_license_num == apps_lookup.hvfhs_license_num)\\\n",
    "    .drop(\"hvfhs_license_num\")\n",
    "\n",
    "shared_trips = shared_trips\\\n",
    "    .groupBy(\"company_name\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"count\")\n",
    "\n",
    "shared_trips.show()\n",
    "\n",
    "# write shared_trips to file\n",
    "try:\n",
    "    shared_trips\\\n",
    "        .repartition(1)\\\n",
    "        .write.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .save(\"/media/felipe/Files/repos/tcc/nyc_data/csv/2020/11/trips_shared_by_app.csv\")\n",
    "except:\n",
    "    print(\"Could not write trips_shared_by_app to file or file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0767c289-4ee1-4b61-80f3-e7e2241f069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find missing lines from pu and do lists\n",
    "all_location_ids = taxi_zones\\\n",
    "    .select(\"LocationID\")\\\n",
    "    .where(\"LocationID < 264\")\\\n",
    "    .withColumn(\"LocationID\", taxi_zones.LocationID.cast(\"int\"))\n",
    "all_location_ids = np.array(all_location_ids.collect()).reshape(-1)\n",
    "\n",
    "location_ids_pu = np.array(trips_by_pu.select(\"PULocationID\").collect()).reshape(-1)\n",
    "location_ids_do = np.array(trips_by_do.select(\"DOLocationID\").collect()).reshape(-1)\n",
    "\n",
    "pu_schema = [\"PULocationID\", \"count\"]\n",
    "do_schema = [\"DOLocationID\", \"count\"]\n",
    "missing_locations_pu = []\n",
    "missing_locations_do = []\n",
    "\n",
    "for location_id in np.nditer(all_location_ids):\n",
    "    if location_id not in location_ids_pu:\n",
    "        missing_locations_pu.append((location_id.flatten().item(0),0))\n",
    "    if location_id not in location_ids_do:\n",
    "        missing_locations_do.append((location_id.flatten().item(0),0))\n",
    "\n",
    "missing_pu_data = spark.createDataFrame(data=missing_locations_pu,schema=pu_schema)\n",
    "trips_by_pu = trips_by_pu\\\n",
    "    .union(missing_pu_data)\\\n",
    "    .orderBy(\"PULocationID\")\n",
    "trips_by_pu_count = np.array(trips_by_pu.select(\"count\").collect()).reshape(-1)\n",
    "\n",
    "missing_do_data = spark.createDataFrame(data=missing_locations_do,schema=do_schema)\n",
    "trips_by_do = trips_by_do\\\n",
    "    .union(missing_do_data)\\\n",
    "    .orderBy(\"DOLocationID\")\n",
    "trips_by_do_count = np.array(trips_by_do.select(\"count\").collect()).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece3fd9b-8e99-4867-b050-141329bb2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------+------+\n",
      "|PULocationID|company_name|company_color| count|\n",
      "+------------+------------+-------------+------+\n",
      "|           1|         Via|      #2AB6E6|   209|\n",
      "|           2|        Uber|      #000000|    25|\n",
      "|           3|        Uber|      #000000| 20768|\n",
      "|           4|        Uber|      #000000| 27771|\n",
      "|           5|        Uber|      #000000|  2239|\n",
      "|           6|        Uber|      #000000|  3966|\n",
      "|           7|        Uber|      #000000| 77975|\n",
      "|           8|        Uber|      #000000|   287|\n",
      "|           9|        Uber|      #000000|  6187|\n",
      "|          10|        Uber|      #000000| 27938|\n",
      "|          11|        Uber|      #000000|  9986|\n",
      "|          12|        Uber|      #000000|   549|\n",
      "|          13|        Uber|      #000000| 29123|\n",
      "|          14|        Uber|      #000000| 48532|\n",
      "|          15|        Uber|      #000000|  4823|\n",
      "|          16|        Uber|      #000000| 14244|\n",
      "|          17|        Uber|      #000000| 82727|\n",
      "|          18|        Uber|      #000000| 56859|\n",
      "|          19|        Uber|      #000000|  4617|\n",
      "|          20|        Uber|      #000000| 31116|\n",
      "|          21|        Uber|      #000000| 24802|\n",
      "|          22|        Uber|      #000000| 32504|\n",
      "|          23|        Uber|      #000000| 11124|\n",
      "|          24|        Uber|      #000000| 13004|\n",
      "|          25|        Uber|      #000000| 39646|\n",
      "|          26|        Uber|      #000000| 53687|\n",
      "|          27|        Uber|      #000000|   507|\n",
      "|          28|        Uber|      #000000| 19052|\n",
      "|          29|        Uber|      #000000| 18226|\n",
      "|          30|        Uber|      #000000|   427|\n",
      "|          31|        Uber|      #000000|  4144|\n",
      "|          32|        Uber|      #000000| 28994|\n",
      "|          33|        Uber|      #000000| 34788|\n",
      "|          34|        Uber|      #000000|  6670|\n",
      "|          35|        Uber|      #000000| 69679|\n",
      "|          36|        Uber|      #000000| 63458|\n",
      "|          37|        Uber|      #000000| 98999|\n",
      "|          38|        Uber|      #000000|  7923|\n",
      "|          39|        Uber|      #000000| 83301|\n",
      "|          40|        Uber|      #000000| 16572|\n",
      "|          41|        Uber|      #000000| 62420|\n",
      "|          42|        Uber|      #000000|100381|\n",
      "|          43|        Uber|      #000000| 12006|\n",
      "|          44|        Uber|      #000000|  3251|\n",
      "|          45|        Uber|      #000000| 13815|\n",
      "|          46|        Uber|      #000000|  1516|\n",
      "|          47|        Uber|      #000000| 39522|\n",
      "|          48|        Uber|      #000000| 60609|\n",
      "|          49|        Uber|      #000000| 50473|\n",
      "|          50|        Uber|      #000000| 40992|\n",
      "|          51|        Uber|      #000000| 33561|\n",
      "|          52|        Uber|      #000000| 10053|\n",
      "|          53|        Uber|      #000000| 12276|\n",
      "|          54|        Uber|      #000000|  4023|\n",
      "|          55|        Uber|      #000000| 22484|\n",
      "|          56|        Uber|      #000000| 30319|\n",
      "|          57|        Uber|      #000000|  2183|\n",
      "|          58|        Uber|      #000000|  2882|\n",
      "|          59|        Uber|      #000000|  1642|\n",
      "|          60|        Uber|      #000000| 28219|\n",
      "|          61|        Uber|      #000000|140046|\n",
      "|          62|        Uber|      #000000| 37217|\n",
      "|          63|        Uber|      #000000| 34841|\n",
      "|          64|        Uber|      #000000|  3679|\n",
      "|          65|        Uber|      #000000| 32377|\n",
      "|          66|        Uber|      #000000| 23315|\n",
      "|          67|        Uber|      #000000| 14659|\n",
      "|          68|        Uber|      #000000| 58123|\n",
      "|          69|        Uber|      #000000| 63089|\n",
      "|          70|        Uber|      #000000| 17930|\n",
      "|          71|        Uber|      #000000| 48010|\n",
      "|          72|        Uber|      #000000| 61142|\n",
      "|          73|        Uber|      #000000|  9456|\n",
      "|          74|        Uber|      #000000| 72423|\n",
      "|          75|        Uber|      #000000| 61844|\n",
      "|          76|        Uber|      #000000|115726|\n",
      "|          77|        Uber|      #000000| 34286|\n",
      "|          78|        Uber|      #000000| 49430|\n",
      "|          79|        Uber|      #000000| 97837|\n",
      "|          80|        Uber|      #000000| 54338|\n",
      "|          81|        Uber|      #000000| 25773|\n",
      "|          82|        Uber|      #000000| 67411|\n",
      "|          83|        Uber|      #000000| 17331|\n",
      "|          84|        Uber|      #000000|  3180|\n",
      "|          85|        Uber|      #000000| 34712|\n",
      "|          86|        Uber|      #000000| 13012|\n",
      "|          87|        Uber|      #000000| 43305|\n",
      "|          88|        Uber|      #000000| 12274|\n",
      "|          89|        Uber|      #000000| 82533|\n",
      "|          90|        Uber|      #000000| 49435|\n",
      "|          91|        Uber|      #000000| 54471|\n",
      "|          92|        Uber|      #000000| 36326|\n",
      "|          93|        Uber|      #000000|  1798|\n",
      "|          94|        Uber|      #000000| 27675|\n",
      "|          95|        Uber|      #000000| 48346|\n",
      "|          96|        Uber|      #000000|  1467|\n",
      "|          97|        Uber|      #000000| 55260|\n",
      "|          98|        Uber|      #000000|  7084|\n",
      "|          99|        Uber|      #000000|   230|\n",
      "|         100|        Uber|      #000000| 30324|\n",
      "|         101|        Uber|      #000000|  4414|\n",
      "|         102|        Uber|      #000000| 22827|\n",
      "|         105|        Uber|      #000000|     1|\n",
      "|         105|        Lyft|      #E867CB|     1|\n",
      "|         106|        Uber|      #000000| 13862|\n",
      "|         107|        Uber|      #000000| 55084|\n",
      "|         108|        Uber|      #000000| 13186|\n",
      "|         109|        Uber|      #000000|  4963|\n",
      "|         110|        Uber|      #000000|    10|\n",
      "|         111|        Uber|      #000000|   437|\n",
      "+------------+------------+-------------+------+\n",
      "only showing top 110 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get df with company name instead of license number\n",
    "fhvhv_data_cmp_name = fhvhv_data\\\n",
    "    .join(apps_lookup,\n",
    "          fhvhv_data.hvfhs_license_num == apps_lookup.hvfhs_license_num)\\\n",
    "    .withColumn(\"PULocationID\", fhvhv_data.PULocationID.cast(\"int\"))\\\n",
    "    .withColumn(\"DOLocationID\", fhvhv_data.DOLocationID.cast(\"int\"))\\\n",
    "    .drop(\"hvfhs_license_num\", \"dispatching_base_num\")\n",
    "\n",
    "fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "    .groupBy(\"PULocationID\", \"company_name\", \"company_color\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"PULocationID\")\n",
    "\n",
    "fhvhv_data_cmp_name.createOrReplaceTempView(\"TMP_VIEW\")\n",
    "\n",
    "query = \"SELECT PULocationID, company_name, company_color, count FROM (SELECT *, MAX(count) OVER (PARTITION BY PULocationID) AS maxCount FROM TMP_VIEW) M WHERE count = maxCount\"\n",
    "\n",
    "fhvhv_data_cmp_name = spark.sql(query)\n",
    "\n",
    "# fhvhv_data_cmp_name.show(110)\n",
    "\n",
    "# fixture specific for may\n",
    "fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "    .withColumn(\"PULocationID\", when((col(\"PULocationID\") == 105) & (col(\"company_name\") == 'Lyft'), 104)\\\n",
    "               .otherwise(col(\"PULocationID\")))\n",
    "# fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "#     .withColumn(\"PULocationID\", when((col(\"PULocationID\") == 105) & (col(\"company_name\") == 'Juno'), 103)\\\n",
    "#                .otherwise(col(\"PULocationID\")))\n",
    "# fixture specific for may\n",
    "\n",
    "# find missing PULocationID in fhvhv_data_cmp_name\n",
    "pu_schema = [\"PULocationID\", \"company_name\", \"company_color\", \"count\"]\n",
    "missing_locations_pu = []\n",
    "\n",
    "location_ids_pu = np.array(fhvhv_data_cmp_name.select(\"PULocationID\").collect()).reshape(-1)\n",
    "\n",
    "for location_id in np.nditer(all_location_ids):\n",
    "    if location_id not in location_ids_pu:\n",
    "        missing_locations_pu.append((location_id.flatten().item(0),'No data','#808080',np.nan))\n",
    "\n",
    "missing_pu_data = spark.createDataFrame(data=missing_locations_pu,schema=pu_schema)\n",
    "fhvhv_data_cmp_name = fhvhv_data_cmp_name\\\n",
    "    .union(missing_pu_data)\\\n",
    "    .orderBy(\"PULocationID\")\n",
    "\n",
    "fhvhv_data_cmp_color = np.array(fhvhv_data_cmp_name.select('company_color').collect()).reshape(-1)\n",
    "fhvhv_data_cmp_name_name = np.array(fhvhv_data_cmp_name.select('company_name').collect()).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c978f81-cd35-4efa-80c2-3814a02b52ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (264) does not match length of index (263)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-963588f03b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Trips_by_PULocationID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrips_by_pu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Trips_by_DOLocationID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrips_by_do_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Top_Company_Color'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhvhv_data_cmp_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Top_Company_Name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhvhv_data_cmp_name_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \"\"\"\n\u001b[1;32m   3241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         raise ValueError(\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (264) does not match length of index (263)"
     ]
    }
   ],
   "source": [
    "# add cols to gdf\n",
    "gdf['Trips_by_PULocationID'] = trips_by_pu_count\n",
    "gdf['Trips_by_DOLocationID'] = trips_by_do_count\n",
    "gdf['Top_Company_Color'] = fhvhv_data_cmp_color\n",
    "gdf['Top_Company_Name'] = fhvhv_data_cmp_name_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08a829-c5bf-4722-b379-a1b0d376d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PU trip heatmap\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.title('NYC, Nov 2020')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gdf.plot(cmap='coolwarm',\n",
    "         column='Trips_by_PULocationID',\n",
    "         ax=ax,\n",
    "         legend=True,\n",
    "         legend_kwds={'label': \"Pickups by Zone\", 'orientation': \"vertical\"})\n",
    "plt.savefig('/media/felipe/Files/repos/tcc/nyc_data/imgs/2020-11_PU.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42c563-ab44-4ca2-8fd7-cb938bac3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot DO trip heatmap\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.title('NYC, Nov 2020')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gdf.plot(cmap='coolwarm',\n",
    "         column='Trips_by_DOLocationID',\n",
    "         ax=ax,\n",
    "         legend=True,\n",
    "         legend_kwds={'label': \"Dropoffs by Zone\", 'orientation': \"vertical\"})\n",
    "plt.savefig('/media/felipe/Files/repos/tcc/nyc_data/imgs/2020-11_DO.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbba7b-9bfe-413b-aeda-ec9a18507a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top companies\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.title('Most used app by NYC zone, Nov 2020')\n",
    "uber_patch = mpatches.Patch(color='#000000', label='Uber')\n",
    "lyft_patch = mpatches.Patch(color='#E867CB', label='Lyft')\n",
    "via_patch = mpatches.Patch(color='#2AB6E6', label='Via')\n",
    "juno_patch = mpatches.Patch(color='#29509F', label='Juno')\n",
    "no_data_patch = mpatches.Patch(color='#808080', label='No data')\n",
    "plt.legend(handles=[uber_patch,lyft_patch,via_patch,juno_patch], loc='upper left')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gdf.plot(color=gdf['Top_Company_Color'],\n",
    "         ax=ax,\n",
    "         legend=True)\n",
    "plt.savefig('/media/felipe/Files/repos/tcc/nyc_data/imgs/2020-11_App.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbcc06-0466-42d0-88f5-b98200a927dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
